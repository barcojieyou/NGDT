{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  },
  "title": "NGD-T_KFAC_CIFAR10_demo"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGD‑T with layerwise K‑FAC (CIFAR‑10) — short demo\n",
    "\n",
    "This notebook implements:\n",
    "- patch‑based K‑FAC factor accumulation for `Conv2d` and `Linear` layers,\n",
    "- per‑layer pseudoinverse via eigendecomposition with caching (recompute every `K` steps),\n",
    "- NGD‑T thermodynamic regulator using global geometric norm \\(\\Delta_F\\),\n",
    "- hybrid updates (natural‑space + small nullspace Euclidean fallback),\n",
    "- diagnostics logging and visualization: training loss, `eta_T`, predicted dissipation `Q_pred`.\n",
    "\n",
    "Run cells in order. Adjust hyperparameters in the config cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Setup and imports\n",
    "import time, math\n",
    "from collections import OrderedDict, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Config and model\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 3\n",
    "LR_BASE = 1.0\n",
    "Q_BUDGET = 1e-3\n",
    "ETA_MIN = 1e-6\n",
    "ETA_MAX = 1.0\n",
    "EPS = 1e-8\n",
    "EMA_DECAY = 0.95\n",
    "DAMPING = 1e-3\n",
    "USE_DAMPING = False\n",
    "TOL = 1e-8\n",
    "KFAC_EIG_UPDATE = 20\n",
    "KFAC_FACTOR_UPDATE = 1\n",
    "ETA_NULL_RATIO = 0.01\n",
    "SEED = 0\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=True)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 256, bias=True)\n",
    "        self.fc2 = nn.Linear(256, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data loaders (torchvision)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# K-FAC layer state and eig helpers\n",
    "class KFACLayerState:\n",
    "    def __init__(self, module, layer_type, ema_decay=0.95, device=\"cpu\"):\n",
    "        self.module = module\n",
    "        self.layer_type = layer_type\n",
    "        self.ema_decay = ema_decay\n",
    "        self.device = device\n",
    "        self.A = None\n",
    "        self.G = None\n",
    "        self.A_plus = None\n",
    "        self.G_plus = None\n",
    "        self.A_eigvals = None\n",
    "        self.A_eigvecs = None\n",
    "        self.G_eigvals = None\n",
    "        self.G_eigvecs = None\n",
    "        self.A_mask = None\n",
    "        self.G_mask = None\n",
    "        self._acts_raw = None\n",
    "        self._backprops_raw = None\n",
    "        self.handle_forward = None\n",
    "        self.handle_backward = None\n",
    "\n",
    "    def register_hooks(self):\n",
    "        module = self.module\n",
    "        def forward_hook(mod, inp, out):\n",
    "            x = inp[0].detach()\n",
    "            self._acts_raw = x.to(self.device)\n",
    "        def backward_hook(mod, grad_in, grad_out):\n",
    "            gy = grad_out[0].detach()\n",
    "            self._backprops_raw = gy.to(self.device)\n",
    "        self.handle_forward = module.register_forward_hook(forward_hook)\n",
    "        self.handle_backward = module.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        if self.handle_forward is not None:\n",
    "            self.handle_forward.remove()\n",
    "            self.handle_forward = None\n",
    "        if self.handle_backward is not None:\n",
    "            self.handle_backward.remove()\n",
    "            self.handle_backward = None\n",
    "\n",
    "    def update_factors(self, damping=0.0):\n",
    "        if self._acts_raw is None or self._backprops_raw is None:\n",
    "            return\n",
    "        if self.layer_type == \"conv\":\n",
    "            x = self._acts_raw\n",
    "            gy = self._backprops_raw\n",
    "            B, C_in, H, W = x.shape\n",
    "            _, C_out, H_out, W_out = gy.shape\n",
    "            kh, kw = self.module.kernel_size\n",
    "            stride = self.module.stride\n",
    "            padding = self.module.padding\n",
    "            x_patches = torch.nn.functional.unfold(x, kernel_size=(kh, kw), padding=padding, stride=stride)\n",
    "            Bp, K, L = x_patches.shape\n",
    "            x_patches = x_patches.permute(0, 2, 1).contiguous().view(Bp * L, K)\n",
    "            gy_patches = gy.permute(0, 2, 3, 1).contiguous().view(Bp * L, C_out)\n",
    "            A_batch = (x_patches.t() @ x_patches) / float(x_patches.shape[0])\n",
    "            G_batch = (gy_patches.t() @ gy_patches) / float(gy_patches.shape[0])\n",
    "            if self.A is None:\n",
    "                self.A = A_batch.detach().clone()\n",
    "            else:\n",
    "                self.A = self.ema_decay * self.A + (1.0 - self.ema_decay) * A_batch.detach()\n",
    "            if self.G is None:\n",
    "                self.G = G_batch.detach().clone()\n",
    "            else:\n",
    "                self.G = self.ema_decay * self.G + (1.0 - self.ema_decay) * G_batch.detach()\n",
    "            if damping > 0.0:\n",
    "                self.A = self.A + damping * torch.eye(self.A.shape[0], device=self.A.device)\n",
    "                self.G = self.G + damping * torch.eye(self.G.shape[0], device=self.G.device)\n",
    "            self._acts_raw = None\n",
    "            self._backprops_raw = None\n",
    "        elif self.layer_type == \"linear\":\n",
    "            x = self._acts_raw\n",
    "            gy = self._backprops_raw\n",
    "            A_batch = (x.t() @ x) / float(x.shape[0])\n",
    "            G_batch = (gy.t() @ gy) / float(gy.shape[0])\n",
    "            if self.A is None:\n",
    "                self.A = A_batch.detach().clone()\n",
    "            else:\n",
    "                self.A = self.ema_decay * self.A + (1.0 - self.ema_decay) * A_batch.detach()\n",
    "            if self.G is None:\n",
    "                self.G = G_batch.detach().clone()\n",
    "            else:\n",
    "                self.G = self.ema_decay * self.G + (1.0 - self.ema_decay) * G_batch.detach()\n",
    "            if damping > 0.0:\n",
    "                self.A = self.A + damping * torch.eye(self.A.shape[0], device=self.A.device)\n",
    "                self.G = self.G + damping * torch.eye(self.G.shape[0], device=self.G.device)\n",
    "            self._acts_raw = None\n",
    "            self._backprops_raw = None\n",
    "\n",
    "def build_kfac_state(model, device, ema_decay=0.95):\n",
    "    kfac_state = OrderedDict()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            st = KFACLayerState(module, \"conv\", ema_decay=ema_decay, device=device)\n",
    "            st.register_hooks()\n",
    "            kfac_state[name] = st\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            st = KFACLayerState(module, \"linear\", ema_decay=ema_decay, device=device)\n",
    "            st.register_hooks()\n",
    "            kfac_state[name] = st\n",
    "    return kfac_state\n",
    "\n",
    "def symmetric_eig_pseudoinverse_torch(mat, tol=1e-8, damping=None):\n",
    "    mat = 0.5 * (mat + mat.t())\n",
    "    eigvals, eigvecs = torch.linalg.eigh(mat)\n",
    "    eigvals = eigvals.flip(0)\n",
    "    eigvecs = eigvecs.flip(1)\n",
    "    sigma_max = max(float(eigvals.max().item()), 1e-12)\n",
    "    tau = max(sigma_max * tol, 1e-12)\n",
    "    if damping is not None and damping > 0.0:\n",
    "        inv_diag = 1.0 / (eigvals + damping)\n",
    "        retained_mask = eigvals > 0.0\n",
    "    else:\n",
    "        inv_diag = torch.zeros_like(eigvals)\n",
    "        retained_mask = eigvals > tau\n",
    "        inv_diag[retained_mask] = 1.0 / eigvals[retained_mask]\n",
    "    mat_plus = (eigvecs * inv_diag.unsqueeze(0)) @ eigvecs.t()\n",
    "    return mat_plus, eigvecs, eigvals, retained_mask\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Precondition, apply updates, and diagnostics\n",
    "def precondition_and_apply_updates(model, kfac_state, q_budget, eta0, eta_min, eta_max, eps,\n",
    "                                   tol, damping, use_damping, eta_null_ratio):\n",
    "    layer_entries = []\n",
    "    total_delta_F = 0.0\n",
    "    for name, st in kfac_state.items():\n",
    "        module = st.module\n",
    "        params = []\n",
    "        grads = []\n",
    "        if hasattr(module, \"weight\") and module.weight is not None:\n",
    "            params.append(module.weight)\n",
    "            grads.append(module.weight.grad if module.weight.grad is not None else torch.zeros_like(module.weight))\n",
    "        if hasattr(module, \"bias\") and module.bias is not None:\n",
    "            params.append(module.bias)\n",
    "            grads.append(module.bias.grad if module.bias.grad is not None else torch.zeros_like(module.bias))\n",
    "        if len(params) == 0:\n",
    "            continue\n",
    "        g_flat = torch.cat([g.contiguous().view(-1) for g in grads]).detach()\n",
    "        if st.A is None or st.G is None:\n",
    "            g_nat_flat = g_flat.clone()\n",
    "            retained_mask_A = None\n",
    "            retained_mask_G = None\n",
    "        else:\n",
    "            if st.A_plus is None or st.G_plus is None:\n",
    "                if use_damping:\n",
    "                    st.A_plus, st.A_eigvecs, st.A_eigvals, st.A_mask = symmetric_eig_pseudoinverse_torch(st.A, tol=tol, damping=damping)\n",
    "                    st.G_plus, st.G_eigvecs, st.G_eigvals, st.G_mask = symmetric_eig_pseudoinverse_torch(st.G, tol=tol, damping=damping)\n",
    "                else:\n",
    "                    st.A_plus, st.A_eigvecs, st.A_eigvals, st.A_mask = symmetric_eig_pseudoinverse_torch(st.A, tol=tol, damping=None)\n",
    "                    st.G_plus, st.G_eigvecs, st.G_eigvals, st.G_mask = symmetric_eig_pseudoinverse_torch(st.G, tol=tol, damping=None)\n",
    "            if st.layer_type == \"linear\":\n",
    "                out, inp = module.weight.shape\n",
    "                grad_mat = g_flat[:out*inp].view(out, inp)\n",
    "                precond_mat = st.G_plus @ grad_mat @ st.A_plus\n",
    "                g_nat_weight = precond_mat.contiguous().view(-1)\n",
    "                if module.bias is not None:\n",
    "                    bias_grad = g_flat[out*inp:]\n",
    "                    if bias_grad.numel() == st.G_plus.shape[0]:\n",
    "                        bias_nat = (st.G_plus @ bias_grad.view(-1,1)).view(-1)\n",
    "                    else:\n",
    "                        bias_nat = bias_grad\n",
    "                    g_nat_flat = torch.cat([g_nat_weight, bias_nat])\n",
    "                else:\n",
    "                    g_nat_flat = g_nat_weight\n",
    "            elif st.layer_type == \"conv\":\n",
    "                out, inp, kh, kw = module.weight.shape\n",
    "                K = inp * kh * kw\n",
    "                grad_weight = g_flat[:out * K].view(out, K)\n",
    "                precond_mat = st.G_plus @ grad_weight @ st.A_plus\n",
    "                g_nat_weight = precond_mat.contiguous().view(-1)\n",
    "                if module.bias is not None:\n",
    "                    bias_grad = g_flat[out * K:]\n",
    "                    if bias_grad.numel() == st.G_plus.shape[0]:\n",
    "                        bias_nat = (st.G_plus @ bias_grad.view(-1,1)).view(-1)\n",
    "                    else:\n",
    "                        bias_nat = bias_grad\n",
    "                    g_nat_flat = torch.cat([g_nat_weight, bias_nat])\n",
    "                else:\n",
    "                    g_nat_flat = g_nat_weight\n",
    "            else:\n",
    "                g_nat_flat = g_flat.clone()\n",
    "            retained_mask_A = st.A_mask\n",
    "            retained_mask_G = st.G_mask\n",
    "        delta_F_layer = float((g_flat @ g_nat_flat).item())\n",
    "        total_delta_F += delta_F_layer\n",
    "        layer_entries.append((name, st, params, grads, g_flat, g_nat_flat, retained_mask_A, retained_mask_G))\n",
    "    total_delta_F = max(total_delta_F, eps)\n",
    "    eta_T = eta0 * (q_budget / (total_delta_F + eps))\n",
    "    eta_T = max(min(eta_T, eta_max), eta_min)\n",
    "    for (name, st, params, grads, g_flat, g_nat_flat, maskA, maskG) in layer_entries:\n",
    "        r = g_flat - g_nat_flat\n",
    "        eta_null = eta_null_ratio * eta_T\n",
    "        delta_flat = -eta_T * g_nat_flat - eta_null * r\n",
    "        idx = 0\n",
    "        for p in params:\n",
    "            n = p.numel()\n",
    "            d = delta_flat[idx:idx+n].view_as(p)\n",
    "            p.data.add_(d)\n",
    "            idx += n\n",
    "    diagnostics = {\"total_delta_F\": total_delta_F, \"eta_T\": eta_T, \"layers\": len(layer_entries)}\n",
    "    return diagnostics\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Training loop with logging\n",
    "def test(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0.0\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss_sum += float(loss.item())\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    acc = 100.0 * correct / total\n",
    "    avg_loss = loss_sum / total\n",
    "    return acc, avg_loss\n",
    "\n",
    "model = SmallCNN(num_classes=10).to(device)\n",
    "model.train()\n",
    "kfac_state = build_kfac_state(model, device=device, ema_decay=EMA_DECAY)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "logs = defaultdict(list)\n",
    "step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    t0 = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        epoch_loss += float(loss.item()) * inputs.size(0)\n",
    "        if step % KFAC_FACTOR_UPDATE == 0:\n",
    "            for st in kfac_state.values():\n",
    "                st.update_factors(damping=DAMPING if USE_DAMPING else 0.0)\n",
    "        if step % KFAC_EIG_UPDATE == 0:\n",
    "            for st in kfac_state.values():\n",
    "                if st.A is not None and st.G is not None:\n",
    "                    if USE_DAMPING:\n",
    "                        st.A_plus, st.A_eigvecs, st.A_eigvals, st.A_mask = symmetric_eig_pseudoinverse_torch(st.A, tol=TOL, damping=DAMPING)\n",
    "                        st.G_plus, st.G_eigvecs, st.G_eigvals, st.G_mask = symmetric_eig_pseudoinverse_torch(st.G, tol=TOL, damping=DAMPING)\n",
    "                    else:\n",
    "                        st.A_plus, st.A_eigvecs, st.A_eigvals, st.A_mask = symmetric_eig_pseudoinverse_torch(st.A, tol=TOL, damping=None)\n",
    "                        st.G_plus, st.G_eigvecs, st.G_eigvals, st.G_mask = symmetric_eig_pseudoinverse_torch(st.G, tol=TOL, damping=None)\n",
    "        diagnostics = precondition_and_apply_updates(\n",
    "            model, kfac_state,\n",
    "            q_budget=Q_BUDGET,\n",
    "            eta0=LR_BASE,\n",
    "            eta_min=ETA_MIN,\n",
    "            eta_max=ETA_MAX,\n",
    "            eps=EPS,\n",
    "            tol=TOL,\n",
    "            damping=DAMPING,\n",
    "            use_damping=USE_DAMPING,\n",
    "            eta_null_ratio=ETA_NULL_RATIO\n",
    "        )\n",
    "        logs[\"loss\"].append(float(loss.item()))\n",
    "        logs[\"eta_T\"].append(diagnostics[\"eta_T\"])\n",
    "        logs[\"predicted_Q\"].append(0.5 * (diagnostics[\"eta_T\"]**2) * diagnostics[\"total_delta_F\"])\n",
    "        logs[\"total_delta_F\"].append(diagnostics[\"total_delta_F\"])\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Epoch {epoch} Step {step} Loss {loss.item():.4f} eta_T {diagnostics['eta_T']:.6f} total_delta_F {diagnostics['total_delta_F']:.6e}\")\n",
    "        step += 1\n",
    "    t1 = time.time()\n",
    "    avg_loss = epoch_loss / len(trainset)\n",
    "    print(f\"Epoch {epoch} completed in {t1-t0:.1f}s, avg loss {avg_loss:.4f}\")\n",
    "    acc, test_loss = test(model, testloader)\n",
    "    print(f\"Test accuracy after epoch {epoch}: {acc:.2f}%  test_loss {test_loss:.4f}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Training finished in {total_time/60.0:.2f} minutes\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot diagnostics\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "steps = np.arange(len(logs[\"loss\"]))\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 10), sharex=True)\n",
    "axs[0].plot(steps, logs[\"loss\"], label=\"train loss\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "axs[1].plot(steps, logs[\"eta_T\"], label=\"eta_T\", color=\"C1\")\n",
    "axs[1].set_ylabel(\"eta_T\")\n",
    "axs[1].legend()\n",
    "axs[2].plot(steps, logs[\"predicted_Q\"], label=\"predicted_Q\", color=\"C2\")\n",
    "axs[2].set_ylabel(\"predicted_Q\")\n",
    "axs[2].set_xlabel(\"training step\")\n",
    "axs[2].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Optional: save model checkpoint\n",
    "torch.save(model.state_dict(), \"ngd_t_kfac_cifar_model.pth\")\n",
    "print(\"Model saved to ngd_t_kfac_cifar_model.pth\")\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "nbformat_minor": 5
}
